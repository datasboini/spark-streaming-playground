{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StackOverflow Dataset Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Case : Create cross platform static dataset for query Analysis\n",
    "\n",
    "## Requirements  \n",
    "\n",
    "- Use stack overflow dataset from [https://snap.stanford.edu/data/sx-stackoverflow.html](https://snap.stanford.edu/data/sx-stackoverflow.html)\n",
    "- Convert the data to parquet format\n",
    "- Create a external table with Hive meta store\n",
    "- Demonstrate sample queries in SparkSQL, ThriftServer and Hive\n",
    "\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Implementation \n",
    "- Explore teh data @ [https://archive.org/details/stackexchange](https://archive.org/details/stackexchange)\n",
    "- Download the data from [https://archive.org/download/stackexchange](https://archive.org/download/stackexchange)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-04-03 19:55:39--  https://ia600107.us.archive.org/27/items/stackexchange/readme.txt\n",
      "Resolving ia600107.us.archive.org (ia600107.us.archive.org)... 207.241.227.247\n",
      "Connecting to ia600107.us.archive.org (ia600107.us.archive.org)|207.241.227.247|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4683 (4.6K) [text/plain]\n",
      "Saving to: ‘/home/mageswarand/ssp/data/downloads/stackoverflow/datascience/readme.txt’\n",
      "\n",
      "/home/mageswarand/s 100%[===================>]   4.57K  --.-KB/s    in 0s      \n",
      "\n",
      "2020-04-03 19:55:40 (257 MB/s) - ‘/home/mageswarand/ssp/data/downloads/stackoverflow/datascience/readme.txt’ saved [4683/4683]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !mkdir -p  ~/ssp/data/downloads/stackoverflow/datascience/meta\n",
    "# !mkdir -p  ~/ssp/data/downloads/stackoverflow/datascience/data\n",
    "# !axel -n 4 https://archive.org/download/stackexchange/datascience.meta.stackexchange.com.7z -o ~/ssp/data/downloads/stackoverflow/datascience/meta/datascience.meta.stackexchange.com.7z\n",
    "# !axel -n 4 https://archive.org/download/stackexchange/datascience.stackexchange.com.7z -o ~/ssp/data/downloads/stackoverflow/datascience/data/datascience.stackexchange.com.7z\n",
    "# !wget https://ia600107.us.archive.org/27/items/stackexchange/readme.txt -O ~/ssp/data/downloads/stackoverflow/datascience/readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data  meta  readme.txt\n"
     ]
    }
   ],
   "source": [
    "!ls ~/ssp/data/downloads/stackoverflow/datascience/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
      "p7zip Version 16.02 (locale=en_IN,Utf16=on,HugeFiles=on,64 bits,12 CPUs Intel(R) Core(TM) i7-8750H CPU @ 2.20GHz (906EA),ASM,AES-NI)\n",
      "\n",
      "Scanning the drive for archives:\n",
      "  0M Sca        1 file, 672641 bytes (657 KiB)\n",
      "\n",
      "Extracting archive: datascience.meta.stackexchange.com.7z\n",
      "--\n",
      "Path = datascience.meta.stackexchange.com.7z\n",
      "Type = 7z\n",
      "Physical Size = 672641\n",
      "Headers Size = 293\n",
      "Method = BZip2\n",
      "Solid = +\n",
      "Blocks = 1\n",
      "\n",
      "    Everything is Ok\n",
      "\n",
      "Files: 8\n",
      "Size:       3964741\n",
      "Compressed: 672641\n"
     ]
    }
   ],
   "source": [
    "!cd ~/ssp/data/downloads/stackoverflow/datascience/meta && 7z x datascience.meta.stackexchange.com.7z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
      "p7zip Version 16.02 (locale=en_IN,Utf16=on,HugeFiles=on,64 bits,12 CPUs Intel(R) Core(TM) i7-8750H CPU @ 2.20GHz (906EA),ASM,AES-NI)\n",
      "\n",
      "Scanning the drive for archives:\n",
      "  0M Sca        1 file, 48515842 bytes (47 MiB)\n",
      "\n",
      "Extracting archive: datascience.stackexchange.com.7z\n",
      "--\n",
      "Path = datascience.stackexchange.com.7z\n",
      "Type = 7z\n",
      "Physical Size = 48515842\n",
      "Headers Size = 326\n",
      "Method = BZip2\n",
      "Solid = +\n",
      "Blocks = 4\n",
      "\n",
      "      6% 1 - Comments.xm                     11% 2 - PostHistory.x                       16% 2 - PostHistory.x                       21% 2 - PostHistory.x                       26% 2 - PostHistory.x                       32% 2 - PostHistory.x                       37% 2 - PostHistory.x                       43% 2 - PostHistory.x                       49% 2 - PostHistory.x                       55% 4 - Posts.x                 59% 4 - Posts.x                 64% 4 - Posts.x                 69% 4 - Posts.x                 75% 4 - Posts.x                 80% 4 - Posts.x                 85% 6 - Users.x                 90% 6 - Users.x                 96% 7 - Votes.x                Everything is Ok\n",
      "\n",
      "Files: 8\n",
      "Size:       262337955\n",
      "Compressed: 48515842\n"
     ]
    }
   ],
   "source": [
    "!cd ~/ssp/data/downloads/stackoverflow/datascience/data && 7z x datascience.stackexchange.com.7z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿ - Format: 7zipped\n",
      " - Files:\n",
      "   - **badges**.xml\n",
      "       - UserId, e.g.: \"420\"\n",
      "       - Name, e.g.: \"Teacher\"\n",
      "       - Date, e.g.: \"2008-09-15T08:55:03.923\"\n",
      "   - **comments**.xml\n",
      "       - Id\n",
      "       - PostId\n",
      "       - Score\n",
      "       - Text, e.g.: \"@Stu Thompson: Seems possible to me - why not try it?\"\n",
      "       - CreationDate, e.g.:\"2008-09-06T08:07:10.730\"\n",
      "       - UserId\n",
      "   - **posts**.xml\n",
      "       - Id\n",
      "       - PostTypeId\n",
      "          - 1: Question\n",
      "          - 2: Answer\n",
      "       - ParentID (only present if PostTypeId is 2)\n",
      "       - AcceptedAnswerId (only present if PostTypeId is 1)\n",
      "       - CreationDate\n",
      "       - Score\n",
      "       - ViewCount\n",
      "       - Body\n",
      "       - OwnerUserId\n",
      "       - LastEditorUserId\n",
      "       - LastEditorDisplayName=\"Jeff Atwood\"\n",
      "       - LastEditDate=\"2009-03-05T22:28:34.823\"\n",
      "       - LastActivityDate=\"2009-03-11T12:51:01.480\"\n",
      "       - CommunityOwnedDate=\"2009-03-11T12:51:01.480\"\n",
      "       - ClosedDate=\"2009-03-11T12:51:01.480\"\n",
      "       - Title=\n",
      "       - Tags=\n",
      "       - AnswerCount\n",
      "       - CommentCount\n",
      "       - FavoriteCount\n",
      "   - **posthistory**.xml\n",
      "\t   - Id\n",
      "\t   - PostHistoryTypeId\n",
      "\t\t\t- 1: Initial Title - The first title a question is asked with.\n",
      "\t\t\t- 2: Initial Body - The first raw body text a post is submitted with.\n",
      "\t\t\t- 3: Initial Tags - The first tags a question is asked with.\n",
      "\t\t\t- 4: Edit Title - A question's title has been changed.\n",
      "\t\t\t- 5: Edit Body - A post's body has been changed, the raw text is stored here as markdown.\n",
      "\t\t\t- 6: Edit Tags - A question's tags have been changed.\n",
      "\t\t\t- 7: Rollback Title - A question's title has reverted to a previous version.\n",
      "\t\t\t- 8: Rollback Body - A post's body has reverted to a previous version - the raw text is stored here.\n",
      "\t\t\t- 9: Rollback Tags - A question's tags have reverted to a previous version.\n",
      "\t\t\t- 10: Post Closed - A post was voted to be closed.\n",
      "\t\t\t- 11: Post Reopened - A post was voted to be reopened.\n",
      "\t\t\t- 12: Post Deleted - A post was voted to be removed.\n",
      "\t\t\t- 13: Post Undeleted - A post was voted to be restored.\n",
      "\t\t\t- 14: Post Locked - A post was locked by a moderator.\n",
      "\t\t\t- 15: Post Unlocked - A post was unlocked by a moderator.\n",
      "\t\t\t- 16: Community Owned - A post has become community owned.\n",
      "\t\t\t- 17: Post Migrated - A post was migrated.\n",
      "\t\t\t- 18: Question Merged - A question has had another, deleted question merged into itself.\n",
      "\t\t\t- 19: Question Protected - A question was protected by a moderator\n",
      "\t\t\t- 20: Question Unprotected - A question was unprotected by a moderator\n",
      "\t\t\t- 21: Post Disassociated - An admin removes the OwnerUserId from a post.\n",
      "\t\t\t- 22: Question Unmerged - A previously merged question has had its answers and votes restored.\n",
      "\t\t- PostId\n",
      "\t\t- RevisionGUID: At times more than one type of history record can be recorded by a single action.  All of these will be grouped using the same RevisionGUID\n",
      "\t\t- CreationDate: \"2009-03-05T22:28:34.823\"\n",
      "\t\t- UserId\n",
      "\t\t- UserDisplayName: populated if a user has been removed and no longer referenced by user Id\n",
      "\t\t- Comment: This field will contain the comment made by the user who edited a post\n",
      "\t\t- Text: A raw version of the new value for a given revision\n",
      "\t\t\t- If PostHistoryTypeId = 10, 11, 12, 13, 14, or 15  this column will contain a JSON encoded string with all users who have voted for the PostHistoryTypeId\n",
      "\t\t\t- If PostHistoryTypeId = 17 this column will contain migration details of either \"from <url>\" or \"to <url>\"\n",
      "\t\t- CloseReasonId\n",
      "\t\t\t- 1: Exact Duplicate - This question covers exactly the same ground as earlier questions on this topic; its answers may be merged with another identical question.\n",
      "\t\t\t- 2: off-topic\n",
      "\t\t\t- 3: subjective\n",
      "\t\t\t- 4: not a real question\n",
      "\t\t\t- 7: too localized\n",
      "   - **postlinks**.xml\n",
      "     - Id\n",
      "     - CreationDate\n",
      "     - PostId\n",
      "     - RelatedPostId\n",
      "     - PostLinkTypeId\n",
      "       - 1: Linked\n",
      "       - 3: Duplicate\n",
      "   - **users**.xml\n",
      "     - Id\n",
      "     - Reputation\n",
      "     - CreationDate\n",
      "     - DisplayName\n",
      "     - EmailHash\n",
      "     - LastAccessDate\n",
      "     - WebsiteUrl\n",
      "     - Location\n",
      "     - Age\n",
      "     - AboutMe\n",
      "     - Views\n",
      "     - UpVotes\n",
      "     - DownVotes\n",
      "   - **votes**.xml\n",
      "     - Id\n",
      "     - PostId\n",
      "     - VoteTypeId\n",
      "        - ` 1`: AcceptedByOriginator\n",
      "        - ` 2`: UpMod\n",
      "        - ` 3`: DownMod\n",
      "        - ` 4`: Offensive\n",
      "        - ` 5`: Favorite - if VoteTypeId = 5 UserId will be populated\n",
      "        - ` 6`: Close\n",
      "        - ` 7`: Reopen\n",
      "        - ` 8`: BountyStart\n",
      "        - ` 9`: BountyClose\n",
      "        - `10`: Deletion\n",
      "        - `11`: Undeletion\n",
      "        - `12`: Spam\n",
      "        - `13`: InformModerator\n",
      "     - CreationDate\n",
      "     - UserId (only for VoteTypeId 5)\n",
      "     - BountyAmount (only for VoteTypeId 9)"
     ]
    }
   ],
   "source": [
    "!cat ~/ssp/data/downloads/stackoverflow/datascience/readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Badges.xml\t\t\t  PostHistory.xml  Tags.xml\n",
      "Comments.xml\t\t\t  PostLinks.xml    Users.xml\n",
      "datascience.stackexchange.com.7z  Posts.xml\t   Votes.xml\n"
     ]
    }
   ],
   "source": [
    "!ls ~/ssp/data/downloads/stackoverflow/datascience/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, isnull, desc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '../../libs/spark-xml_2.12-0.5.0.jar': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls ../../libs/spark-xml_2.12-0.5.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_master = \"spark://IMCHLT276:7077\"\n",
    "spark = SparkSession.builder. \\\n",
    "                appName(\"twitter_stream\"). \\\n",
    "                config(\"spark.jars\", \"../../libs/spark-xml_2.11-0.9.0.jar\"). \\\n",
    "                config(\"spark.executor.memory\", \"3g\"). \\\n",
    "                config(\"spark.executor.cores\", 3). \\\n",
    "                config(\"spark.cores.max\", 6). \\\n",
    "                master(spark_master). \\\n",
    "                enableHiveSupport(). \\\n",
    "                getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.43.98:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://IMCHLT276:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>twitter_stream</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f14750f6a20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Badges.xml\t\t\t  PostHistory.xml  Tags.xml\n",
      "Comments.xml\t\t\t  PostLinks.xml    Users.xml\n",
      "datascience.stackexchange.com.7z  Posts.xml\t   Votes.xml\n"
     ]
    }
   ],
   "source": [
    "!ls ~/ssp/data/downloads/stackoverflow/datascience/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File : Badges.xml\n",
      "+------+--------------------+---+--------------+---------+-------+\n",
      "|_Class|               _Date|_Id|         _Name|_TagBased|_UserId|\n",
      "+------+--------------------+---+--------------+---------+-------+\n",
      "|     3|2014-05-13T23:06:...|  1|      Informed|    false|      1|\n",
      "|     3|2014-05-13T23:11:...|  2|Autobiographer|    false|      2|\n",
      "|     3|2014-05-13T23:20:...|  3|Autobiographer|    false|      4|\n",
      "|     3|2014-05-13T23:20:...|  4|Autobiographer|    false|      5|\n",
      "|     3|2014-05-13T23:20:...|  5|Autobiographer|    false|      8|\n",
      "+------+--------------------+---+--------------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "File : PostHistory.xml\n",
      "+--------+--------------------+---+------------------+-------+--------------------+--------------------+----------------+-------+\n",
      "|_Comment|       _CreationDate|_Id|_PostHistoryTypeId|_PostId|       _RevisionGUID|               _Text|_UserDisplayName|_UserId|\n",
      "+--------+--------------------+---+------------------+-------+--------------------+--------------------+----------------+-------+\n",
      "|    null|2014-05-13T23:58:...|  7|                 2|      5|009bca93-fce2-44e...|I've always been ...|            null|      5|\n",
      "|    null|2014-05-13T23:58:...|  8|                 1|      5|009bca93-fce2-44e...|How can I do simp...|            null|      5|\n",
      "|    null|2014-05-13T23:58:...|  9|                 3|      5|009bca93-fce2-44e...|  <machine-learning>|            null|      5|\n",
      "|    null|2014-05-14T00:11:...| 12|                 2|      7|ea5a5642-ed30-43e...|As a researcher a...|            null|     36|\n",
      "|    null|2014-05-14T00:11:...| 13|                 1|      7|ea5a5642-ed30-43e...|What open-source ...|            null|     36|\n",
      "+--------+--------------------+---+------------------+-------+--------------------+--------------------+----------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "File : Tags.xml\n",
      "+------+--------------+---+----------------+-----------+\n",
      "|_Count|_ExcerptPostId|_Id|        _TagName|_WikiPostId|\n",
      "+------+--------------+---+----------------+-----------+\n",
      "|    28|           105|  1|     definitions|        104|\n",
      "|  7217|          4909|  2|machine-learning|       4908|\n",
      "|   424|            66|  3|         bigdata|         65|\n",
      "|   972|            80|  5|     data-mining|         79|\n",
      "|    82|          8960|  6|       databases|       8959|\n",
      "+------+--------------+---+----------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "File : Comments.xml\n",
      "+--------------------+---+-------+------+--------------------+----------------+-------+\n",
      "|       _CreationDate|_Id|_PostId|_Score|               _Text|_UserDisplayName|_UserId|\n",
      "+--------------------+---+-------+------+--------------------+----------------+-------+\n",
      "|2014-05-14T00:23:...|  5|      5|     9|this is a super t...|            null|     34|\n",
      "|2014-05-14T00:38:...|  6|      7|     4|List questions ar...|            null|     51|\n",
      "|2014-05-14T01:16:...|  9|      7|     3|This question app...|            null|     66|\n",
      "|2014-05-14T02:00:...| 12|     15|     3|This question is ...|            null|     51|\n",
      "|2014-05-14T02:16:...| 13|     10|     2|Nice one, @Nichol...|            null|     24|\n",
      "+--------------------+---+-------+------+--------------------+----------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "File : PostLinks.xml\n",
      "+--------------------+---+-----------+-------+--------------+\n",
      "|       _CreationDate|_Id|_LinkTypeId|_PostId|_RelatedPostId|\n",
      "+--------------------+---+-----------+-------+--------------+\n",
      "|2014-05-14T07:56:...|  9|          1|     14|             1|\n",
      "|2014-05-15T01:46:...| 50|          1|     75|            71|\n",
      "|2014-05-20T17:42:...|172|          1|     59|            41|\n",
      "|2014-06-13T16:44:...|387|          1|    361|            61|\n",
      "|2014-06-13T16:58:...|392|          1|     61|           361|\n",
      "+--------------------+---+-----------+-------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "File : Users.xml\n",
      "+--------------------+----------+--------------------+------------+----------+---+--------------------+------------------+--------------------+-----------+--------+------+--------------------+\n",
      "|            _AboutMe|_AccountId|       _CreationDate|_DisplayName|_DownVotes|_Id|     _LastAccessDate|         _Location|    _ProfileImageUrl|_Reputation|_UpVotes|_Views|         _WebsiteUrl|\n",
      "+--------------------+----------+--------------------+------------+----------+---+--------------------+------------------+--------------------+-----------+--------+------+--------------------+\n",
      "|<p>Hi, I'm not re...|        -1|2014-05-13T21:29:...|   Community|      1383| -1|2014-05-13T21:29:...|on the server farm|                null|          1|     800|     0|http://meta.stack...|\n",
      "|\n",
      "\n",
      "<p>Developer at...|     37099|2014-05-13T22:58:...|   Adam Lear|         0|  1|2019-06-11T19:12:...|      New York, NY|https://i.stack.i...|        101|       0|   552|                null|\n",
      "|<p>Developer on t...|         2|2014-05-13T22:59:...|Geoff Dalgas|         0|  2|2019-09-03T19:10:...|     Corvallis, OR|https://i.stack.i...|        101|       0|     9|http://stackoverf...|\n",
      "|<p>I'm a student ...|   3046327|2014-05-13T23:15:...|  hichris123|         3|  3|2020-02-08T03:11:...|              null|https://i.stack.i...|        101|       1|     8|                null|\n",
      "|<p>I work with <a...|      2365|2014-05-13T23:16:...| Ben Collins|         0|  4|2014-08-04T15:25:...| Republic of Texas|                null|        101|       0|     9|http://benjaminco...|\n",
      "+--------------------+----------+--------------------+------------+----------+---+--------------------+------------------+--------------------+-----------+--------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "File : Posts.xml\n",
      "+-----------------+------------+--------------------+--------------------+-------------+-------------------+--------------------+--------------+---+--------------------+--------------------+----------------------+-----------------+-----------------+------------+---------+-----------+------+--------------------+--------------------+----------+\n",
      "|_AcceptedAnswerId|_AnswerCount|               _Body|         _ClosedDate|_CommentCount|_CommunityOwnedDate|       _CreationDate|_FavoriteCount|_Id|   _LastActivityDate|       _LastEditDate|_LastEditorDisplayName|_LastEditorUserId|_OwnerDisplayName|_OwnerUserId|_ParentId|_PostTypeId|_Score|               _Tags|              _Title|_ViewCount|\n",
      "+-----------------+------------+--------------------+--------------------+-------------+-------------------+--------------------+--------------+---+--------------------+--------------------+----------------------+-----------------+-----------------+------------+---------+-----------+------+--------------------+--------------------+----------+\n",
      "|             null|           1|<p>I've always be...|2014-05-14T14:40:...|            1|               null|2014-05-13T23:58:...|             1|  5|2014-05-14T00:36:...|                null|                  null|             null|             null|           5|     null|          1|     9|  <machine-learning>|How can I do simp...|       671|\n",
      "|               10|           3|<p>As a researche...|2014-05-14T08:40:...|            4|               null|2014-05-14T00:11:...|             1|  7|2014-05-16T13:45:...|2014-05-16T13:45:...|                  null|               97|             null|          36|     null|          1|     4|<education><open-...|What open-source ...|       438|\n",
      "|             null|        null|<p>Not sure if th...|                null|            0|               null|2014-05-14T00:36:...|          null|  9|2014-05-14T00:36:...|                null|                  null|             null|             null|          51|        5|          2|     5|                null|                null|      null|\n",
      "|             null|        null|<p>One book that'...|                null|            1|               null|2014-05-14T00:53:...|          null| 10|2014-05-14T00:53:...|                null|                  null|             null|             null|          22|        7|          2|    13|                null|                null|      null|\n",
      "|               29|           4|<p>I am sure data...|                null|            1|               null|2014-05-14T01:25:...|             6| 14|2014-06-20T17:36:...|2014-06-17T16:17:...|                  null|              322|             null|          66|     null|          1|    22|<data-mining><def...|Is Data Science t...|      1645|\n",
      "+-----------------+------------+--------------------+--------------------+-------------+-------------------+--------------------+--------------+---+--------------------+--------------------+----------------------+-----------------+-----------------+------------+---------+-----------+------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "File : Votes.xml\n",
      "+-------------+--------------------+---+-------+-------+-----------+\n",
      "|_BountyAmount|       _CreationDate|_Id|_PostId|_UserId|_VoteTypeId|\n",
      "+-------------+--------------------+---+-------+-------+-----------+\n",
      "|         null|2014-05-13T00:00:...|  1|      1|   null|          2|\n",
      "|         null|2014-05-13T00:00:...|  2|      1|   null|          2|\n",
      "|         null|2014-05-13T00:00:...|  3|      3|   null|          2|\n",
      "|         null|2014-05-13T00:00:...|  5|      3|   null|          2|\n",
      "|         null|2014-05-13T00:00:...|  6|      1|   null|          2|\n",
      "+-------------+--------------------+---+-------+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_data(path, debug=True):\n",
    "    \"\"\"\n",
    "    Loads all the Stackoverflow files as PySpark DataFrame. Column names are prefixed with `_` by default.\n",
    "    :path : Root directory of the zip extracted folder\n",
    "    returns: Dictionary of DataFrames where each key(lowercase) represents the respective XML file\n",
    "    \"\"\"\n",
    "    files = [\"Badges.xml\", \"PostHistory.xml\", \"Tags.xml\", \"Comments.xml\", \"PostLinks.xml\", \"Users.xml\", \"Posts.xml\", \"Votes.xml\"]\n",
    "    dfs = {}\n",
    "    for key in files:\n",
    "        full_path = path + key\n",
    "        key_ = key.lower().replace(\".xml\", \"\")\n",
    "        dfs[key_] = spark.read.format(\"com.databricks.spark.xml\").option(\"rootTag\", \"Tags\").option(\"attributePrefix\", \"_\").option(\"rowTag\", \"row\").load(full_path)\n",
    "        if debug:\n",
    "            print(f\"File : {key}\")\n",
    "            dfs[key_].show(5)\n",
    "    return dfs\n",
    "\n",
    "path = \"file:///\" + os.path.expanduser(\"~/ssp/data/downloads/stackoverflow/datascience/data/\")\n",
    "dfs = load_data(path=path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find top 5 Posts with more answers and find what those questions are?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = dfs[\"posts\"].where(~isnull(col(\"_AnswerCount\"))).sort(desc(\"_AnswerCount\")).select([\"_AnswerCount\", \"_Body\"]).collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>One of the common problems in data science is gathering data from various sources in a somehow cleaned (semi-structured) format and combining metrics from various sources for making a higher level analysis. Looking at the other people's effort, especially other questions on this site, it appears that many people in this field are doing somewhat repetitive work. For example analyzing tweets, facebook posts, Wikipedia articles etc. is a part of a lot of big data problems.</p>\n",
      "\n",
      "<p>Some of these data sets are accessible using public APIs provided by the provider site, but usually, some valuable information or metrics are missing from these APIs and everyone has to do the same analyses again and again. For example, although clustering users may depend on different use cases and selection of features, but having a base clustering of Twitter/Facebook users can be useful in many Big Data applications, which is neither provided by the API nor available publicly in independent data sets.</p>\n",
      "\n",
      "<p>Is there any index or publicly available data set hosting site containing valuable data sets that can be reused in solving other big data problems? I mean something like GitHub (or a group of sites/public datasets or at least a comprehensive listing) for the data science. If not, what are the reasons for not having such a platform for data science? The commercial value of data, need to frequently update data sets, ...? Can we not have an open-source model for sharing data sets devised for data scientists?</p>\n",
      "\n",
      "-------------------------------------------\n",
      "<p>I'm using Neural Networks to solve different Machine learning problems. I'm using Python and <a href=\"http://pybrain.org/\" rel=\"noreferrer\">pybrain</a> but this library is almost discontinued. Are there other good alternatives in Python?</p>\n",
      "\n",
      "-------------------------------------------\n",
      "<p>I'm just starting to develop a <a href=\"https://en.wikipedia.org/wiki/Machine_learning\" rel=\"noreferrer\">machine learning</a> application for academic purposes. I'm currently using <strong>R</strong> and training myself in it. However, in a lot of places, I have seen people using <strong>Python</strong>.</p>\n",
      "\n",
      "<p>What are people using in academia and industry, and what is the recommendation?</p>\n",
      "\n",
      "-------------------------------------------\n",
      "<p>When writing a paper / making a presentation about a topic which is about neural networks, one usually visualizes the networks architecture.</p>\n",
      "\n",
      "<p>What are good / simple ways to visualize common architectures automatically?</p>\n",
      "\n",
      "-------------------------------------------\n",
      "<p>This is a similar question like the <a href=\"https://stats.stackexchange.com/questions/1904/statistics-conferences\">Statistics Conferences question at CrossValidated</a></p>\n",
      "\n",
      "<p>What are the most significant annual Data Science conferences?</p>\n",
      "\n",
      "<p>Rules:</p>\n",
      "\n",
      "<ul>\n",
      "<li>Include a link to the conference</li>\n",
      "<li>Please include links for the talks (be it youtube, the conference site or some other video streaming site)</li>\n",
      "</ul>\n",
      "\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for q in questions:\n",
    "    print(q[\"_Body\"])\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stay tuned for more updates!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/getting-started-with-data-analytics-using-jupyter-notebooks-pyspark-and-docker-57c1aaab2408"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType, IntegerType, ArrayType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema():\n",
    "    # define the schema to extract the data we are interested\n",
    "    # https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object\n",
    "    urls = ArrayType(StructType(). \\\n",
    "                     add(\"expanded_url\", StringType(), True))\n",
    "\n",
    "    media = ArrayType(StructType(). \\\n",
    "                      add(\"media_url\", StringType(), True). \\\n",
    "                      add(\"media_url_https\", StringType(), True))\n",
    "\n",
    "    # Tweet -> Entities{} -> Urls[] -> Media[]\n",
    "    entities = StructType(). \\\n",
    "        add(\"urls\", urls, True). \\\n",
    "        add(\"media\", media)\n",
    "\n",
    "    schema = StructType(). \\\n",
    "        add('id_str', StringType(), False). \\\n",
    "        add('created_at', StringType(), False). \\\n",
    "        add('source', StringType(), False). \\\n",
    "        add('text', StringType(), False). \\\n",
    "        add('extended_tweet', StructType().add(\"full_text\", StringType(), True), True). \\\n",
    "        add('entities', entities, False). \\\n",
    "        add('retweeted_status', StructType().add('extended_tweet', StructType().add(\"full_text\", StringType(), True), True).add('user', StructType().add('description', StringType())), True). \\\n",
    "        add('geo', StringType(), True). \\\n",
    "        add('retweet_count', IntegerType(), True)\n",
    "\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files_path = \"file:///opt/vlab/gyan42/spark-streaming-playground/data/streams/tweets/\"\n",
    "sdf = spark.readStream.format(\"json\").schema(get_schema()).load(test_files_path).repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_acc = spark.sparkContext.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================\n",
      "\n",
      "\n",
      "+-------------------+--------------------+--------------------+--------------------+--------------+--------------------+--------------------+----+-------------+\n",
      "|             id_str|          created_at|              source|                text|extended_tweet|            entities|    retweeted_status| geo|retweet_count|\n",
      "+-------------------+--------------------+--------------------+--------------------+--------------+--------------------+--------------------+----+-------------+\n",
      "|1246751216496816129|Sun Apr 05 10:47:...|<a href=\"http://t...|RT @smolkjd: afte...|          null|[[], [[http://pbs...|[, ['The standard...|null|            0|\n",
      "|1246751191511584769|Sun Apr 05 10:47:...|<a href=\"http://t...|RT @morganhousel:...|          null|               [[],]|[, [@collabfund\n",
      "\n",
      "...|null|            0|\n",
      "+-------------------+--------------------+--------------------+--------------------+--------------+--------------------+--------------------+----+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "6001\n"
     ]
    }
   ],
   "source": [
    "def foreach_batch_function(df, epoch_id):\n",
    "    # Transform and write batchDF\n",
    "    global count_acc\n",
    "    print(\"=================================================================================\\n\\n\")\n",
    "    df.show(2, True)\n",
    "    \n",
    "    count = df.count()\n",
    "    count_acc += count\n",
    "\n",
    "sdf.writeStream.foreachBatch(foreach_batch_function).start().processAllAvailable()\n",
    "print(count_acc.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lgoerl.github.io/spark-overflow-analysis.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
