{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Optimization for Advances Users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"contents\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [RDD](#rdd)\n",
    "- [Partitions](#partitions)\n",
    "- [Joins](#joins)\n",
    "- [Data Serialisation](#dataserialisation)\n",
    "- [UDF](#udf)\n",
    "- [Analyse Execution plan](#executionplan)\n",
    "- [Data Skew](#data_skew)\n",
    "- [Cache](#cache)\n",
    "- [Storage](#storage)\n",
    "- [Misc](#misc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rdd'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD\n",
    "\n",
    "\n",
    "* Avoid `groupBy` and prefer `reduceBy`\n",
    "\n",
    "  ```scala\n",
    "  val a = sc.parallelize(List(\"dog\", \"cat\", \"owl\", \"gnu\", \"ant\"), 2)\n",
    "  val b = a.map(x => (x.length, x))\n",
    "  b.reduceByKey(_ + _).collect\n",
    "  // b.groupByKey().mapValues(_.mkString(\"\")).collect()\n",
    "  // Array[(Int, String)] = Array((3,dogcatowlgnuant))\n",
    "  \n",
    "  val a = sc.parallelize(List(\"dog\", \"tiger\", \"lion\", \"cat\", \"panther\", \"eagle\"), 2)\n",
    "  val b = a.map(x => (x.length, x))\n",
    "  b.reduceByKey(_ + _).collect\n",
    "  //// b.groupByKey().mapValues(_.mkString(\"\")).collect()\n",
    "  // Array[(Int, String)] = Array((4,lion), (3,dogcat), (7,panther), (5,tigereagle))\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='partitions'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitions - That Detemines the parallelism\n",
    "\n",
    "  * Avoid spills and utilize all the cores\n",
    "  * At input level\n",
    "    * `spark.default.parallelism`(don't use)\n",
    "    * `spark.sql.files.maxPartitionBytes`\n",
    "      * Use default partition size **128MB** , unless\n",
    "        * Increase parallelism\n",
    "        * Heavily nested/repeated data\n",
    "        * Generating data\n",
    "        * Source structure is not optimal\n",
    "        * UDFs\n",
    "      * ![](images/paritionfilesize.png)\n",
    "  * At shuffle level\n",
    "    * `sparl.sql.shuffle.partitions`\n",
    "    * Default maximum size is **2GB**\n",
    "\n",
    "  - At ouput level\n",
    "\n",
    "    - Coalesce to shrink number of partitions\n",
    "\n",
    "    - Repartition to increase the number of partitions\n",
    "\n",
    "    - ```python\n",
    "      df.write.option(\"maxRecordsPerFile\", n)\n",
    "      df.coalesce(n).write...\n",
    "      df.repartition(n).write...\n",
    "      df.repartition(n, [colA, ...]).write...\n",
    "      spark.sql.shuffle.partitions(n)\n",
    "      df.localCheckpoint(...).repartition(n).write...\n",
    "      df.localCheckpoint(...).coalesce(n).write...\n",
    "      ```\n",
    "\n",
    "    - ```\n",
    "      Write Data Size = 14.7GB\n",
    "      Desired File Size = 1500MB\n",
    "      Max write stage parallelism = 10\n",
    "      # Say we have 96 cores\n",
    "      96 – 10 == 86 cores idle during write\n",
    "      \n",
    "      14.7/96 cores = ~ 160 MB of parition size\n",
    "      spark.sql.files.maxPartitionBytes = 160 * 1024 * 1024\n",
    "      So now we use all 96 cores\n",
    "      ```\n",
    "\n",
    "    - ![](images/spark_partition_file_size.png)\n",
    "\n",
    "  - **Never rely on default partition size of `200` at any cost**\n",
    "\n",
    "    - ```\n",
    "      Say if you have data size of 210GB\n",
    "      number_partitions = 210000MB / 200MB = 1050\n",
    "      # irrespective of number of cores\n",
    "      spark.conf.set(\"Spark.sql.shuffle.paritions\", 1050)\n",
    "      # say if the cluster has 2000 cores\n",
    "      spark.conf.set(\"Spark.sql.shuffle.paritions\", 2000)\n",
    "      ```\n",
    "\n",
    "    - ![](images/spill.png)\n",
    "\n",
    "    - ![](images/spill_corrected.png)\n",
    "\n",
    "[<< Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"joins\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins\n",
    "\n",
    "  * SortMerge Join – Both sides are lrage\n",
    "\n",
    "  * **Broadcast DataFrame** Join when one side is small\n",
    "\n",
    "    * Default size is 10MB\n",
    "* Spark SQL uses **broadcast join** (aka **broadcast hash join**) instead of hash join to optimize join queries when the size of one side data is below [spark.sql.autoBroadcastJoinThreshold](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-properties.html#spark.sql.autoBroadcastJoinThreshold).\n",
    "    * Broadcast allows to send a read-only variable cached on each node once, rather than sending a copy for all tasks. \n",
    "    * Automatic If:\n",
    "        (one side < spark.sql.autoBroadcastJoinThreshold) (default 10m)\n",
    "      * Risks\n",
    "        - Not Enough Driver Memory\n",
    "        - DF > spark.driver.maxResultSize\n",
    "        - DF > Single Executor Available Working Memory\n",
    "      * ![](images/broadcat_explained.png)\n",
    "    \n",
    "  * Requires to use Hive and its metastore for auto detection of table size\n",
    "    \n",
    "    ```scala\n",
    "    leftDF.join(broadcast(rightDF))\n",
    "    ```\n",
    "    \n",
    "  * **Put the Largest Dataset on the Left**\n",
    "  \n",
    "    * When you’re joining together two datasets where one is smaller than the other, put the larger dataset on the “Left”:\n",
    "    * When Spark shuffles data for the join, it keeps the data you specified on  the left static on the executors and transfers the data you designed on  the right between the executors. If the data that’s on the right, that’s being transferred, is larger, then the serialization and transfer of  the data will take longer.\n",
    "  \n",
    "  ```\n",
    "  val joinedDF = largerDF.leftJoin(smallerDF, largerDF(\"id\") === smallerDF(\"some_id\"))\n",
    "  ```\n",
    "    * Be aware of `null` values \n",
    "  \n",
    "        * Consider sample query where we are joining on highly null columns\n",
    "  \n",
    "        * ![](images/null_counts.png)\n",
    "  \n",
    "          ```sql\n",
    "          select * from  order_tbl orders left join customer_tbl  customer\n",
    "                on orders.customer_id = customer.customer_id\n",
    "                left join delivery_tbl delivery\n",
    "                on  orders.delivery_id = delivery.delivery_id\n",
    "          ```\n",
    "  \n",
    "        * In above query, 199/200 tasks would complete quite fast and then probably gets stuck on the last task. \n",
    "  \n",
    "        * ***Reason for the above behavior:\\*** Let’s say that we have asked Spark to join two DataFrames — TABLE1 and  TABLE2. When Spark executes this code, internally it performs the  default Shuffle Hash Join (Exchange hashpartitioning).\n",
    "  \n",
    "          ```\n",
    "          +- Exchange hashpartitioning(delivery_id#22L, 400)\n",
    "                         +- *(6) Project [delivery_id#22L]\n",
    "          ```\n",
    "  \n",
    "          In this process, Spark hashes the join column and sorts it. And then it  tries to keep the records with same hashes in both partitions on the  same executor hence all the null values of the table will go to one  executor and spark gets into a continuous loop of shuffling and garbage  collection with no success.\n",
    "  \n",
    "      * Solution is split the tables into two parts, one with null values(i.e all the values) and without null values. \n",
    "  \n",
    "      * Use table without null values for join and then union them with full data\n",
    "  \n",
    "      * This way null values won't participate in the join\n",
    "  \n",
    "      * ```sql\n",
    "        CREATE TABLE order_tbl_customer_id_not_null as select * from order_tbl where customer_id is not null;\n",
    "        \n",
    "        CREATE TABLE order_tbl_customer_id_null as select * from order_tbl where customer_id is null;\n",
    "        \n",
    "        --Rewrite query\n",
    "        select orders.customer_id\n",
    "        from  order_tbl_customer_id_not_null orders left join customer_tbl  customer \n",
    "                 on orders.customer_id = customer.customer_id\n",
    "        union all\n",
    "        select ord.customer_id from order_tbl_customer_id_null ord;\n",
    "        ```\n",
    "        \n",
    "    * Coalesce vs repartition\n",
    "\n",
    "      In the literature, it’s often mentioned that *coalesce* should be preferred over *repartition* to reduce the number of partitions because it avoids a shuffle step in some cases.\n",
    "\n",
    "      But ***coalesce\\* has some limitations** (outside the scope of this article): it cannot increase the number of partitions and may generate skew partitions.\n",
    "\n",
    "      Here is one case where a **repartition should be preferred**. In this case, we filter most of a dataset.\n",
    "\n",
    "      ```\n",
    "      df.doSth().coalesce(10).write(…)\n",
    "      ```\n",
    "\n",
    "\n",
    "\n",
    "      ![img](images/coalesce.png)\n",
    "\n",
    "      The good point about *coalesce* is that it avoids a shuffle. However, all the computation is done by only 10 tasks.\n",
    "\n",
    "      This is due to the fact that **the number of tasks depends on the number of partitions of the output of the stage**, each one computing a big bunch of data. So a maximum of 10 nodes will perform the computation.\n",
    "\n",
    "      ```\n",
    "      df.doSth().repartition(10).write(…)\n",
    "      ```\n",
    "\n",
    "      ![](images/repartition.png)\n",
    "\n",
    "      Using *repartition* we can see that the total duration is way shorter (a few seconds  instead of 31 minutes). The filtering is done by 200 tasks, each one  working on a small subset of data. It’s also way smoother from a memory  point a view, as we can see in the graph below.\n",
    "\n",
    "      ![](images/repartition_memory.png)\n",
    "  \n",
    "[<< Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dataserialisation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Serialisation\n",
    "\n",
    "  - It is the process of converting the in-memory object to another format  that can be used to store in a file or send over the network.\n",
    "  - Two options\n",
    "    - Java serialization\n",
    "    - Kryo serialization\n",
    "  - Configuration : “spark.serializer” to “org.apache.spark.serializer.KyroSerializer\"\n",
    "\n",
    "[<<< Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"udf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF\n",
    "\n",
    "\n",
    "  * Avoid User defined Function **UDFs** and User Defined Aggregate Funtions **UDAF** \n",
    "  * Prefer in build SQL Functions where ever possible\n",
    "  * Traditional UDFs cannot use Tungsten\n",
    "\n",
    "    * Use `org.apache.spark.sql.functions`\n",
    "    * Use PandasUDFs\n",
    "\n",
    "      * Utilizes Apache Arrow\n",
    "    * Use SparkR UDFs\n",
    "  * https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\n",
    "  * https://fr.slideshare.net/cfregly/advanced-apache-spark-meetup-project-tungsten-nov-12-2015\n",
    "\n",
    "[<< Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"executionplan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Execution plan\n",
    "\n",
    "  - Stages are created when there is a need for shuffle for operations like `join` or `groupBy`\n",
    "\n",
    "  - **Number of stages**\n",
    "\n",
    "    * Thought the Spark engine does pretty good job of optimizing the DAGs for executions, it is also developer responsibility to keep the number of stages under a reasonable number.\n",
    "    * This involves good amount of understanding of APIs and SQL query optimization\n",
    "\n",
    "  - We use the` .explain(true)` command to show the execution plan detailing all the steps (stages) involved for a job. Here is an example:\n",
    "\n",
    "    ![](images/explain.png)\n",
    "    \n",
    "[<<< Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_skew\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Skew\n",
    "\n",
    "  - Common symptoms of data skew are:\n",
    "    - Frozen stages and tasks.\n",
    "      - Especially with the last couple of tasks in a stage\n",
    "    - Low utilization of CPU.\n",
    "    - Out of memory errors.\n",
    "  - Data broadcast during join operation can help with minimize the data skew side effects. Increase the `spark.sql.autoBroadcastJoinThreshold` for Spark to consider tables of bigger size. Make sure enough memory is available in driver and executors\n",
    "  - Salting\n",
    "    - In a SQL join operation, the join key is changed to redistribute data in an even manner so that processing for a partition does not take more  time. This technique is called salting.\n",
    "    \n",
    "    -  Add Column to each side with random int between 0 and `spark.sql.shuffle.partitions` – 1 to both sides\n",
    "    \n",
    "    - Add join clause to include join on generated column above\n",
    "    \n",
    "    - Drop temp columns from result\n",
    "    \n",
    "    - ```scala\n",
    "      df.groupBy(“city”, “state”).agg(<f(x)>).orderBy(col.desc)\n",
    "      val saltVal = random(0, spark.conf.get(org...shuffle.partitions) - 1)\n",
    "      df.withColumn(“salt”, lit(saltVal))\n",
    "      .groupBy(“city”, “state”, “salt”)\n",
    "      .agg(<f(x)>)\n",
    "      .drop(“salt”)\n",
    "      .orderBy(col.desc)\n",
    "      \n",
    "      ```\n",
    "      \n",
    "* **Highly imbalanced dataset**\n",
    "\n",
    "  - To quickly check if everything is ok we review the execution duration of each task and look for **heterogeneous process time**. If one of the tasks is significantly slower than the others it will  extend the overall job duration and waste the resources of the fastest  executors.\n",
    "\n",
    "    ![](images/stage_execution_time.png)\n",
    "    \n",
    "\n",
    "- **COMPUTE STATISTICS** of Tables Before Processing\n",
    "\n",
    "  - Before querying a series of tables, it can be helpful to tell spark to Compute the Statistics of those tables so that the Catalyst Optimizer can come  up with a better plan on how to process the tables.\n",
    "\n",
    "  ```\n",
    "  spark.sql(\"ANALYZE TABLE dbName.tableName COMPUTE STATISTICS\")\n",
    "  ```\n",
    "\n",
    "  - In some cases, Spark doesn’t get everything it needs from just the above  broad COMPUTE STATISTICS call. It also helps to tell Spark to check  specific columns so the Catalyst Optimizer can better check those  columns. It’s recommended to COMPUTE STATISTICS for any columns that are involved in filtering and joining.\n",
    "\n",
    "```\n",
    "spark.sql(\"ANALYZE TABLE dbName.tableName COMPUTE STATISTICS FOR COLUMNS joinColumn, filterColumn\")\n",
    "```\n",
    "\n",
    "- Avoid calling **DataFrame/RDD `count` API** unless it is absolutely necessary. \n",
    "\n",
    "[<<< Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cache\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache\n",
    "\n",
    "* **Inappropriate use of caching**\n",
    "\n",
    "  There is no universal answer when choosing what should be cached. Caching an intermediate result can **dramatically improve performance** and it’s tempting to cache a lot of things. However, due to Spark’s  caching strategy (in-memory then swap to disk) the cache can end up in a slightly slower storage. Also, using that storage space for caching  purposes means that it’s not available for processing. In the end,  caching might cost more than simply reading the *DataFrame*.\n",
    "\n",
    "  In the *Storage* tab of the UI we verify the *Fraction Cached* and also look at the *Size in Memory* and *Size on Disk* distribution.\n",
    "\n",
    "  ![](images/cacahe.png)\n",
    "  \n",
    "[<<< Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"storage\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage\n",
    "\n",
    "* **Pitfalls with AWS S3**\n",
    "\n",
    "  - A simple renaming actually needs to copy and then delete the original file.\n",
    "\n",
    "  - The first [workaround](https://medium.com/@subhojit20_27731/apache-spark-and-amazon-s3-gotchas-and-best-practices-a767242f3d98) when using Spark with S3 as an output it to use this specific configuration:\n",
    "\n",
    "    ```shell\n",
    "    spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version 2 spark.speculation false\n",
    "    ```\n",
    "    \n",
    "- **Data Skipping**\n",
    "  \n",
    "  - **Parquet**\n",
    "    \n",
    "    - Columnar storage format\n",
    "    - Spark can leverage **predicate push down** (i.e. pushing down the filtering closer to the data).\n",
    "    - However, predicate pushdown should be used with extra care. Even if it appears in the execution plan, **it will not always be effective**. For example, a filter push-down does not work on String and Decimal data types (cf[ PARQUET-281](https://issues.apache.org/jira/browse/PARQUET-281)).\n",
    "    \n",
    "  - Hive Partitions\n",
    "  \n",
    "  - Hive **Bucketing**\n",
    "  \n",
    "    * Consider when there is a high hashable column values\n",
    "  \n",
    "    * `spark.sql.sources.bucketing.enabled=True`\n",
    "  \n",
    "    * Set  `hive.enforce.bucketing=false` and `hive.enforce.sorting=false`\n",
    "  \n",
    "    * https://www.slideshare.net/databricks/hive-bucketing-in-apache-spark-with-tejas-patil\n",
    "  \n",
    "    * ```scala\n",
    "      df.write\n",
    "        .bucketBy(numBuckets, \"col1\", ...)\n",
    "        .sortBy(\"col1\", ...)\n",
    "        .saveAsTable(\"/path/table_name\")\n",
    "      ```\n",
    "  \n",
    "    * ```sql\n",
    "      create table table_name(col1 INT, ...)\n",
    "      \tusing parquet\n",
    "      \tCLUSTERED By (col1, ...)\n",
    "      \tSORTED BY(col1, ...)\n",
    "      \tINTO `numBuckets` BUCKETS\n",
    "      ```\n",
    "  \n",
    "    * ![](images/bucketing.png)\n",
    "    \n",
    "[<<< Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"monitoring\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring spark applications\n",
    "\n",
    "  - Spark includes a configurable metrics system based on the [*dropwizard.metrics*](http://metrics.dropwizard.io/) library. It is set up via the Spark configuration. As we already are heavy users of **Graphite** and **Grafana**, we use the provided [Graphite sink](https://www.hammerlab.org/2015/02/27/monitoring-spark-with-graphite-and-grafana/).\n",
    "\n",
    "    The Graphite sink needs to be **used with caution**. This is due to the fact that, for each metric, Graphite creates a fixed-size database to store data points. These zeroed-out [*Whisper*](https://graphite.readthedocs.io/en/latest/whisper.html) files consume quite a lot of disk space.\n",
    "\n",
    "    By default, the application id is added to the namespace, which means that every *spark-submit* operation creates a new metric. Thanks to [SPARK-5847](https://issues.apache.org/jira/browse/SPARK-5847) it is now possible to **mitigate the \\*Whisper\\* behavior** and remove the *spark.app.id* from the namespace.\n",
    "\n",
    "  ```\n",
    "  \tspark.metrics.namespace=$name\n",
    "  ```\n",
    "\n",
    "  \n",
    "  - https://github.com/hammerlab/grafana-spark-dashboards\n",
    "  - https://www.hammerlab.org/2015/02/27/monitoring-spark-with-graphite-and-grafana/\n",
    "  - https://github.com/qubole/sparklens\n",
    "\n",
    "* **Java Garbage Collection**\n",
    "\n",
    "  - Analyse the logs for the memory usage, most likely the problem would be with the data partitions.\n",
    "  - https://spark.apache.org/docs/latest/tuning.html#garbage-collection-tuning\n",
    "  - EMR : https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-debugging.html\n",
    "  - Tool to analyse the logs: https://gceasy.io/\n",
    "\n",
    "  JVM Heap monitoring\n",
    "\n",
    "\n",
    "[<<< Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"misc\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc \n",
    "\n",
    "* Try alternatives for AWS EMR with plain EC2 \n",
    "\n",
    "  - https://github.com/nchammas/flintrock\n",
    "  - https://heather.miller.am/blog/launching-a-spark-cluster-part-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Good reads\n",
    "\n",
    "- https://medium.com/teads-engineering/lessons-learned-while-optimizing-spark-aggregation-jobs-f93107f7867f\n",
    "- https://www.slideshare.net/databricks/apache-spark-coredeep-diveproper-optimization\n",
    "- https://michalsenkyr.github.io/2018/01/spark-performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the respective Gist for this notebook :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'e1957c887cd8ec900f84ae91842636b9'...\n",
      "remote: Enumerating objects: 6, done.\u001b[K\n",
      "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
      "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
      "remote: Total 6 (delta 1), reused 4 (delta 1), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (6/6), done.\n",
      "[master 5d9a9ad] updated\n",
      " 1 file changed, 6 insertions(+), 82 deletions(-)\n",
      "Counting objects: 3, done.\n",
      "Delta compression using up to 12 threads.\n",
      "Compressing objects: 100% (3/3), done.\n",
      "Writing objects: 100% (3/3), 366 bytes | 366.00 KiB/s, done.\n",
      "Total 3 (delta 1), reused 0 (delta 0)\n",
      "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
      "To https://gist.github.com/Mageswaran1989/e1957c887cd8ec900f84ae91842636b9\n",
      "   f8c4b58..5d9a9ad  HEAD -> master\n"
     ]
    }
   ],
   "source": [
    "! rm -rf e1957c887cd8ec900f84ae91842636b9\n",
    "! git clone https://gist.github.com/Mageswaran1989/e1957c887cd8ec900f84ae91842636b9\n",
    "! cp SparkOptimization.ipynb e1957c887cd8ec900f84ae91842636b9/spark_optimization_for_Advanced_users.ipynb  \n",
    "! cd e1957c887cd8ec900f84ae91842636b9 && git add . && git commit -m \"updated\" && git push origin HEAD\n",
    "! rm -rf e1957c887cd8ec900f84ae91842636b9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
