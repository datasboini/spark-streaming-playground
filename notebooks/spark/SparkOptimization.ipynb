{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Optimization for Advances Users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"contents\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rdd'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<< Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='partitions'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"joins\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dataserialisation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"udf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_skew\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cache\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"storage\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"jvm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Java Garbage Collection\n",
    "\n",
    "To more on the topic read [here](https://javarevisited.blogspot.com/2011/04/garbage-collection-in-java.html)\n",
    "\n",
    "<img src=\"images/JavaGarbageCollectionheap.png\" />\n",
    "\n",
    "`spark.executor.extraJavaOptions = -XX:+UseConcMarkSweepGC -XX:+CMSClassUnloadingEnabled -XX:UseG1GC XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThread=20`\n",
    "\n",
    "  - Analyse the logs for the memory usage, most likely the problem would be with the data partitions.\n",
    "  - https://spark.apache.org/docs/latest/tuning.html#garbage-collection-tuning\n",
    "  - EMR : https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-debugging.html\n",
    "  - Tool to analyse the logs: https://gceasy.io/\n",
    "\n",
    "\n",
    "[<<< Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"monitoring\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Monitoring spark applications\n",
    "\n",
    "  - Spark includes a configurable metrics system based on the [*dropwizard.metrics*](http://metrics.dropwizard.io/) library. It is set up via the Spark configuration. As we already are heavy users of **Graphite** and **Grafana**, we use the provided [Graphite sink](https://www.hammerlab.org/2015/02/27/monitoring-spark-with-graphite-and-grafana/).\n",
    "\n",
    "    The Graphite sink needs to be **used with caution**. This is due to the fact that, for each metric, Graphite creates a fixed-size database to store data points. These zeroed-out [*Whisper*](https://graphite.readthedocs.io/en/latest/whisper.html) files consume quite a lot of disk space.\n",
    "\n",
    "    By default, the application id is added to the namespace, which means that every *spark-submit* operation creates a new metric. Thanks to [SPARK-5847](https://issues.apache.org/jira/browse/SPARK-5847) it is now possible to **mitigate the \\*Whisper\\* behavior** and remove the *spark.app.id* from the namespace.\n",
    "\n",
    "  ```\n",
    "  \tspark.metrics.namespace=$name\n",
    "  ```\n",
    "\n",
    "  \n",
    "  - https://github.com/hammerlab/grafana-spark-dashboards\n",
    "  - https://www.hammerlab.org/2015/02/27/monitoring-spark-with-graphite-and-grafana/\n",
    "  - https://github.com/qubole/sparklens\n",
    "\n",
    "\n",
    "[<<< Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"misc\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Misc \n",
    "\n",
    "* Try alternatives for AWS EMR with plain EC2 \n",
    "\n",
    "  - https://github.com/nchammas/flintrock\n",
    "  - https://heather.miller.am/blog/launching-a-spark-cluster-part-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Good reads\n",
    "\n",
    "- https://medium.com/teads-engineering/lessons-learned-while-optimizing-spark-aggregation-jobs-f93107f7867f\n",
    "- https://www.slideshare.net/databricks/apache-spark-coredeep-diveproper-optimization\n",
    "- https://michalsenkyr.github.io/2018/01/spark-performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the respective Gist for this notebook :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gist.github.com/Mageswaran1989/e1957c887cd8ec900f84ae91842636b9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbconvert --to ltex SparkOptimization.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf e1957c887cd8ec900f84ae91842636b9\n",
    "# ! git clone https://gist.github.com/Mageswaran1989/e1957c887cd8ec900f84ae91842636b9\n",
    "# ! cd e1957c887cd8ec900f84ae91842636b9 && rm *\n",
    "# ! cp SparkOptimization.html e1957c887cd8ec900f84ae91842636b9/SparkOptimization.html\n",
    "# ! cd e1957c887cd8ec900f84ae91842636b9 && git add . && git commit -m \"updated\" && git push origin HEAD\n",
    "# ! rm -rf e1957c887cd8ec900f84ae91842636b9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
